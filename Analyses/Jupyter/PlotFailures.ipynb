{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d96d7c-77e4-4296-862a-922f01a53dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSamuel Grant 2024\\n\\nRead failure events and make plots. \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Samuel Grant 2024\n",
    "\n",
    "Read failure events and make plots. \n",
    "\n",
    "Based on PlotFailures.py. Better suited to notebook (so we don't have to load the data each time). \n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7453752-529d-451e-94ea-22db42c4716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "import uproot\n",
    "\n",
    "# Internal libraries\n",
    "sys.path.append(os.path.abspath(\"../PyMacros\"))\n",
    "import Utils as ut\n",
    "from Mu2eEAF import ReadData as rd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb411ed-5dfe-4bc3-ae2d-7dd07c28ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 96 files in list.\n"
     ]
    }
   ],
   "source": [
    "# Get file list\n",
    "fileListPath=\"../Txt/FileLists/MDC2020aeOnExpData.txt\"\n",
    "def ReadFileList(fileListPath):\n",
    "  with open(fileListPath, \"r\") as fileList_:\n",
    "    lines = fileList_.readlines()\n",
    "    lines = [line.strip() for line in lines]  # Remove leading/trailing whitespace\n",
    "  return lines\n",
    "fileList_=ReadFileList(fileListPath)\n",
    "print(f\"---> {len(fileList_)} files in list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e29d151-7029-41f7-b1e0-75d3051a22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get failure info\n",
    "For each file, collect the events. \n",
    "\n",
    "This takes quite a while (2-3 minutes maybe).\n",
    "I spent a full day trying to multithread this, it blows up my memory. \n",
    "'''\n",
    "def GetFailures(fileList_, PEs=10, layers=\"3\", particle=\"all\", cut=\"singles\"):\n",
    "\n",
    "     # Collect failure info\n",
    "    failureFilePath = f\"../Txt/MDC2020ae/concatenated/failures_concise/failures_concise_all_{PEs}PEs{layers}Layers_{cut}.csv\"\n",
    "\n",
    "    print(f\"\\n---> Using info file {failureFilePath}.\")\n",
    "    failureInfo_ = pd.read_csv(failureFilePath)\n",
    "\n",
    "    # Collect file list\n",
    "    tags_ = list(set(failureInfo_[\"Tag\"]))\n",
    "    \n",
    "    # Extract the tag from the file name\n",
    "    def ExtractTag(fileName):\n",
    "        parts = fileName.split('.')\n",
    "        if len(parts) > 1:\n",
    "            return parts[-2]\n",
    "        return None\n",
    "\n",
    "    # Filter and sort files based on tags\n",
    "    fileList_ = sorted(\n",
    "        [file for file in fileList_ if ExtractTag(file) in tags_]\n",
    "        , key=lambda file: tags_.index(ExtractTag(file))\n",
    "    )\n",
    "\n",
    "    # testing\n",
    "    # tags_ = tags_[:1]\n",
    "    # fileList_ = fileList_[:1]\n",
    "    \n",
    "    # Bug check\n",
    "    if len(fileList_) == len(tags_):\n",
    "        print(\"\\n---> Collected and sorted failure file names.\")\n",
    "    else:\n",
    "        raise Exception(\"\\n---> len(fileList_) != len(tags_)\")\n",
    "\n",
    "    completedFiles = 0\n",
    "    totalFiles = len(fileList_)\n",
    "\n",
    "    print(f\"\\n---> Iterating through {len(fileList_)} files.\\n\") \n",
    "\n",
    "    # Master array to hold results\n",
    "    data_ = ak.Array([])\n",
    "    \n",
    "    # Iterate through file list.\n",
    "    for tag, fileName in zip(tags_, fileList_): \n",
    "    \n",
    "        # Find failure events\n",
    "        thisFailureInfo_ = failureInfo_[failureInfo_[\"Tag\"] == tag]\n",
    "        outputStr = ( \n",
    "            f\"\\n--->\\n\" \n",
    "            f\"fileName: {fileName}\\n\"\n",
    "            f\"tag: {tag}\\n\"\n",
    "            f\"failures:\\n{thisFailureInfo_}\\n\"\n",
    "            f\"---\"\n",
    "        )\n",
    "        if False: print(outputStr)\n",
    "    \n",
    "        # Read the file\n",
    "        with uproot.open(fileName) as file: \n",
    "            \n",
    "            # file = (fileName, quiet=True)\n",
    "            # Get array\n",
    "            thisData_ = ut.GetData(file)\n",
    "        \n",
    "            if False: print(f\"\\n---> Loaded corresponding data.\\n{thisData_}\")\n",
    "        \n",
    "            if False: print(f\"\\n---> Applying masks.\")\n",
    "        \n",
    "            # Extract unique values from DataFrame\n",
    "            runs_ = set(thisFailureInfo_[\"evtinfo.run\"])\n",
    "            subruns_ = set(thisFailureInfo_[\" evtinfo.subrun\"])\n",
    "            events_ = set(thisFailureInfo_[\" evtinfo.event\"])\n",
    "        \n",
    "            # Construct masks\n",
    "            runCondition = ak.any([thisData_[\"evt\"][\"evtinfo.run\"] == value for value in runs_], axis=0)\n",
    "            subrunCondition = ak.any([thisData_[\"evt\"][\"evtinfo.subrun\"] == value for value in subruns_], axis=0)\n",
    "            eventCondition = ak.any([thisData_[\"evt\"][\"evtinfo.event\"] == value for value in events_], axis=0)\n",
    "        \n",
    "            # Apply masks\n",
    "            thisData_ = thisData_[runCondition & subrunCondition & eventCondition]\n",
    "        \n",
    "            # Append to master array\n",
    "            if False: print(f\"\\n---> Appending failures to master array.\")\n",
    "            data_ = ak.concatenate([data_, thisData_], axis=0)\n",
    "\n",
    "            if False:\n",
    "                print(thisFailureInfo_)\n",
    "                ut.PrintNEvents(data_)\n",
    "    \n",
    "            completedFiles += 1\n",
    "            percentComplete = (completedFiles / totalFiles) * 100\n",
    "            \n",
    "            print(f\"\\r---> Processed {fileName} ({percentComplete:.1f}% complete...)\", end=\"\") \n",
    "\n",
    "    inputEventList = failureInfo_[\" evtinfo.event\"]\n",
    "    outputEventList = ak.flatten(data_[\"evt\"][\"evtinfo.event\"], axis=None)\n",
    "\n",
    "    return data_\n",
    "    \n",
    "    # # print(outputEventList)\n",
    "    # # ut.PrintNEvents(data_)\n",
    "\n",
    "    # if len(inputEventList) == len(outputEventList):\n",
    "    #     print(\"data_ contains the correct number of events!\")\n",
    "    #     print(\"Done!\")\n",
    "    #     print(len(inputEventList), len(outputEventList))\n",
    "    #     return data_\n",
    "    # else:\n",
    "    #     raise ValueError(f\"data_ contains the wrong number of events! {len(outputEventList)} compared with {len(inputEventList)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95ec887c-9668-4cec-8c62-26a12f3b99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PEs=10\n",
    "layers=\"3\"\n",
    "particle=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2e2ab39-1cb2-4233-a89e-31fa8a1e0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Using info file ../Txt/MDC2020ae/concatenated/failures_concise/failures_concise_all_10PEs3Layers_singles.csv.\n",
      "\n",
      "---> Collected and sorted failure file names.\n",
      "\n",
      "---> Iterating through 36 files.\n",
      "\n",
      "---> Processed /exp/mu2e/data/users/sgrant/CRVSim/CosmicCRYExtractedCatTriggered.MDC2020ae_best_v1_3.000/11946817/00/00058/nts.sgrant.CosmicCRYExtractedCatTriggered.MDC2020ae_best_v1_3.001205_00000025.root (100.0% complete...)"
     ]
    }
   ],
   "source": [
    "data_singles_ = GetFailures(fileList_, cut=\"singles\")\n",
    "# ut.PrintNEvents(data_singles_, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0cae004c-4373-4c97-8192-284457beb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Using info file ../Txt/MDC2020ae/concatenated/failures_concise/failures_concise_all_10PEs3Layers_singles_track_cuts.csv.\n",
      "\n",
      "---> Collected and sorted failure file names.\n",
      "\n",
      "---> Iterating through 9 files.\n",
      "\n",
      "---> Processed /exp/mu2e/data/users/sgrant/CRVSim/CosmicCRYExtractedCatTriggered.MDC2020ae_best_v1_3.000/11946817/00/00034/nts.sgrant.CosmicCRYExtractedCatTriggered.MDC2020ae_best_v1_3.001205_00000051.root (100.0% complete...)"
     ]
    }
   ],
   "source": [
    "data_singles_track_cuts_ = GetFailures(fileList_, cut=\"singles_track_cuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "193a2156-57bd-4969-b05f-72ae12b37a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut.PrintNEvents(data_singles_track_cuts_, 1);\n",
    "# ut.PrintNEvents(data_singles_track_cuts_, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aeae7a-d4d7-40e4-9153-43c709f01360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the failure data...\n",
    "# Plot it here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d3b3a482-6029-4b38-8140-6923525ca8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Processed 001205_00000554 (100.0% complete...)\n",
      "---> data_singles_:\n",
      "                Tag evtinfo.run  evtinfo.subrun  evtinfo.event\n",
      "0   001205_00000014        1205            6544         282227\n",
      "1   001205_00000016        1205            3118         422170\n",
      "2   001205_00000016        1205            6413          66312\n",
      "3   001205_00000019        1205              19         454961\n",
      "4   001205_00000019        1205             827         326820\n",
      "5   001205_00000020        1205            2263         473431\n",
      "6   001205_00000025        1205            6734          49366\n",
      "7   001205_00000028        1205            9374         378962\n",
      "8   001205_00000031        1205            6489         185068\n",
      "9   001205_00000036        1205            6304          66812\n",
      "10  001205_00000040        1205              46         329464\n",
      "11  001205_00000051        1205            8523         162615\n",
      "12  001205_00000053        1205            6173          94514\n",
      "13  001205_00000062        1205            3875         238282\n",
      "14  001205_00000062        1205            6145          64216\n",
      "15  001205_00000064        1205            1024          65053\n",
      "16  001205_00000064        1205            5421         292241\n",
      "17  001205_00000064        1205            8188          85434\n",
      "18  001205_00000082        1205            2751         321323\n",
      "19  001205_00000083        1205            7468         384774\n",
      "20  001205_00000089        1205             842         488806\n",
      "21  001205_00000094        1205            3156           8612\n",
      "22  001205_00000099        1205             166         287296\n",
      "23  001205_00000110        1205            1897          82003\n",
      "24  001205_00000110        1205            3452         366363\n",
      "25  001205_00000110        1205            5535          77723\n",
      "26  001205_00000116        1205            5349         183839\n",
      "27  001205_00000117        1205            5933         440038\n",
      "28  001205_00000118        1205            8026         211605\n",
      "29  001205_00000145        1205            4577         360310\n",
      "30  001205_00000150        1205            8186         385240\n",
      "31  001205_00000171        1205            5515         498871\n",
      "32  001205_00000178        1205             572         472115\n",
      "33  001205_00000210        1205             221         188077\n",
      "34  001205_00000210        1205            4135          35896\n",
      "35  001205_00000231        1205            5496         370121\n",
      "36  001205_00000243        1205            1782         187470\n",
      "\n",
      "---> data_singles_track_cuts_:\n",
      "               Tag evtinfo.run  evtinfo.subrun  evtinfo.event\n",
      "0  001205_00000014        1205            6544         282227\n",
      "1  001205_00000020        1205            2263         473431\n",
      "2  001205_00000051        1205            8523         162615\n",
      "3  001205_00000062        1205            3875         238282\n",
      "4  001205_00000064        1205            1024          65053\n",
      "5  001205_00000116        1205            5349         183839\n",
      "6  001205_00000145        1205            4577         360310\n",
      "7  001205_00000178        1205             572         472115\n",
      "8  001205_00000210        1205             221         188077\n"
     ]
    }
   ],
   "source": [
    "# What is more interesting is those that pass the singles cut, but not the track cut.\n",
    "# Although, this good be biased a bit because sometimes the track cuts generate an exception, while the singles do not!\n",
    "# Need to ID a file where this doesn't happen. (See ../Scripts/FindGoodFiles.sh)\n",
    "# 79 good files, written tags to ../Txt/MDC2020ae/GoodFiles/good_files.csv\n",
    "\n",
    "goodFileTags_ = ReadFileList(\"../Txt/MDC2020ae/GoodFiles/good_files.csv\")\n",
    "\n",
    "# Make a dataframe for singles and singles_track_cuts using these files. \n",
    "def CollectGoodFiles(goodFileTags_, PEs=\"10\", layer=\"3\", particle=\"all\", cut=\"singles\"):\n",
    "    baseDir=f\"../Txt/MDC2020ae/failures_concise\"\n",
    "    data_ = pd.DataFrame()\n",
    "    tasks = len(goodFileTags_)\n",
    "    completedTasks = 1\n",
    "    for i, goodFileTag in enumerate(goodFileTags_):\n",
    "        if i == 0: continue # skip header\n",
    "        goodFile = f\"{baseDir}/{goodFileTag}/failures_concise_{particle}_{PEs}PEs{layer}Layers_{cut}.csv\"\n",
    "        thisData_ = pd.read_csv(goodFile)\n",
    "        # Append data to the main DataFrame without resetting the index\n",
    "        if thisData_ is not None: \n",
    "            # Add the tag to the first column\n",
    "            thisData_.insert(0, \"Tag\", goodFileTag)\n",
    "            # Concatenate\n",
    "            data_ = pd.concat([data_, thisData_], ignore_index=True)\n",
    "        completedTasks += 1\n",
    "        percentComplete = (completedTasks / tasks) * 100\n",
    "        print(f\"\\r---> Processed {goodFileTag} ({percentComplete:.1f}% complete...)\", end=\"\") \n",
    "    return data_\n",
    "\n",
    "data_singles_ = CollectGoodFiles(goodFileTags_, PEs=10, cut=\"singles\")\n",
    "data_singles_track_cuts_ = CollectGoodFiles(goodFileTags_, PEs=10, cut=\"singles_track_cuts\")\n",
    "\n",
    "print(f\"\\n---> data_singles_:\\n{data_singles_}\")\n",
    "print(f\"\\n---> data_singles_track_cuts_:\\n{data_singles_track_cuts_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6aa558ff-ca51-4696-9f37-8649638b0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tag evtinfo.run  evtinfo.subrun  evtinfo.event\n",
      "1   001205_00000016        1205            3118         422170\n",
      "2   001205_00000016        1205            6413          66312\n",
      "3   001205_00000019        1205              19         454961\n",
      "4   001205_00000019        1205             827         326820\n",
      "6   001205_00000025        1205            6734          49366\n",
      "7   001205_00000028        1205            9374         378962\n",
      "8   001205_00000031        1205            6489         185068\n",
      "9   001205_00000036        1205            6304          66812\n",
      "10  001205_00000040        1205              46         329464\n",
      "12  001205_00000053        1205            6173          94514\n",
      "14  001205_00000062        1205            6145          64216\n",
      "16  001205_00000064        1205            5421         292241\n",
      "17  001205_00000064        1205            8188          85434\n",
      "18  001205_00000082        1205            2751         321323\n",
      "19  001205_00000083        1205            7468         384774\n",
      "20  001205_00000089        1205             842         488806\n",
      "21  001205_00000094        1205            3156           8612\n",
      "22  001205_00000099        1205             166         287296\n",
      "23  001205_00000110        1205            1897          82003\n",
      "24  001205_00000110        1205            3452         366363\n",
      "25  001205_00000110        1205            5535          77723\n",
      "27  001205_00000117        1205            5933         440038\n",
      "28  001205_00000118        1205            8026         211605\n",
      "30  001205_00000150        1205            8186         385240\n",
      "31  001205_00000171        1205            5515         498871\n",
      "34  001205_00000210        1205            4135          35896\n",
      "35  001205_00000231        1205            5496         370121\n",
      "36  001205_00000243        1205            1782         187470\n"
     ]
    }
   ],
   "source": [
    "# Now find entries unique to singles_track_cuts_ \n",
    "# Merge DataFrames with an left join to get rows unique to singles_\n",
    "# I run the singles filter before the track cuts filter. \n",
    "# This means that some events will pass the singles filter, and then be removed by the track cuts filter.\n",
    "# These events are unique to singles. \n",
    "# We can look at the in the event display to see what happens in these cases. \n",
    "\n",
    "# Merge\n",
    "data_merged_ = pd.merge(data_singles_, data_singles_track_cuts_, how=\"left\", indicator=True)\n",
    "# Find unique to singles events\n",
    "unique_to_singles_ = data_merged_[data_merged_['_merge'] == 'left_only']\n",
    "# Drop the _merge column\n",
    "unique_to_singles_ = unique_to_singles_.drop('_merge', axis=1)\n",
    "print(unique_to_singles_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6dd62-1b67-4125-b5c5-7aa3085d284a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mu2eEnv",
   "language": "python",
   "name": "mu2eenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
